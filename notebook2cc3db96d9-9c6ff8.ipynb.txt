{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11634350,"sourceType":"datasetVersion","datasetId":7299592},{"sourceId":11736883,"sourceType":"datasetVersion","datasetId":7368194}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install necessary packages if running for the first time\n!pip install numpy torch nibabel nilearn scikit-image pandas openpyxl","metadata":{"execution":{"iopub.status.busy":"2025-04-25T12:08:58.227338Z","iopub.execute_input":"2025-04-25T12:08:58.227602Z","iopub.status.idle":"2025-04-25T12:09:01.588031Z","shell.execute_reply.started":"2025-04-25T12:08:58.227576Z","shell.execute_reply":"2025-04-25T12:09:01.587304Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"!pip install numpy torch nibabel nilearn scikit-image pandas openpyxl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:21:52.438831Z","iopub.execute_input":"2025-05-09T02:21:52.439104Z","iopub.status.idle":"2025-05-09T02:23:13.748582Z","shell.execute_reply.started":"2025-05-09T02:21:52.439084Z","shell.execute_reply":"2025-05-09T02:23:13.747576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pylab inline\nimport numpy as np\nimport torch\nimport os\nimport pandas as pd\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch import autograd\nfrom torch.autograd import Variable\nimport nibabel as nib\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom skimage.transform import resize\nfrom nilearn import plotting\nimport warnings\n\n# Suppress specific warnings (optional)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='torch.nn.functional') \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:13.750808Z","iopub.execute_input":"2025-05-09T02:23:13.751124Z","iopub.status.idle":"2025-05-09T02:23:18.249182Z","shell.execute_reply.started":"2025-05-09T02:23:13.751103Z","shell.execute_reply":"2025-05-09T02:23:18.248624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pylab inline\nimport numpy as np\nimport torch\nimport os\nimport pandas as pd\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch import autograd\nfrom torch.autograd import Variable\nimport nibabel as nib\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom nilearn import plotting\nfrom skimage.transform import resize\nimport warnings\n\n\nwarnings.filterwarnings('ignore') # Ignore warnings for cleaner output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:18.24993Z","iopub.execute_input":"2025-05-09T02:23:18.250292Z","iopub.status.idle":"2025-05-09T02:23:18.25902Z","shell.execute_reply.started":"2025-05-09T02:23:18.250273Z","shell.execute_reply":"2025-05-09T02:23:18.257665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DATASET_ROOT = '/kaggle/input/data-yte/not_skull_stripped'\n# PARTICIPANTS_FILE = '/kaggle/input/data-yte/not_skull_stripped/participants.xlsx'\nDATASET_ROOT = \"/kaggle/input/mri-dataset\"\nCHECKPOINT_DIR = './checkpoint_mydataset'\nIMAGE_SAVE_DIR = './generated_images_mydataset'\n\nBATCH_SIZE = 8      # Adjust based on GPU memory\nlatent_dim = 1000    # Latent dimension size\nIMG_SIZE = 64        # Target image size (e.g., 64x64x64)\ngpu = True           # Use GPU if available\nworkers = 4          # Number of dataloader workers\n\nLAMBDA = 10          # Gradient penalty lambda\n_eps = 1e-15         # Small epsilon for numerical stability\nLR = 0.0002          # Learning rate for Adam optimizers\nBETA1 = 0.5          # Beta1 for Adam optimizers (common for GANs)\nBETA2 = 0.999        # Beta2 for Adam optimizers\n\ng_iter = 1           # Generator iterations per main loop\nd_iter = 1           # Discriminator iterations per main loop\ncd_iter = 1          # Code Discriminator iterations per main loop\nTOTAL_ITER = 50000  # Total training iterations\nVISUALIZATION_INTERVAL = 100 # How often to visualize images\nSAVE_INTERVAL = 5000 # How often to save model checkpoints\n\n# Create directories if they don't exist\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(IMAGE_SAVE_DIR, exist_ok=True)\n\n# Setup device\nif gpu and torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:18.260003Z","iopub.execute_input":"2025-05-09T02:23:18.260276Z","iopub.status.idle":"2025-05-09T02:23:22.139908Z","shell.execute_reply.started":"2025-05-09T02:23:18.26025Z","shell.execute_reply":"2025-05-09T02:23:22.139251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyMRIDataset(Dataset):\n    def __init__(self, data_dir, img_size=64, augmentation=False):\n        self.img_size = img_size\n        self.augmentation = augmentation\n        self.image_paths = []\n\n        if not os.path.exists(data_dir):\n            raise FileNotFoundError(f\"Directory {data_dir} not found\")\n\n        # Lọc tất cả các file .nii hoặc .nii.gz trong thư mục\n        for fname in os.listdir(data_dir):\n            if fname.endswith(\".nii\") or fname.endswith(\".nii.gz\"):\n                full_path = os.path.join(data_dir, fname)\n                self.image_paths.append(full_path)\n        # SÔ LƯỢNG ẢNH TRAINING     \n        self.image_paths = self.image_paths[:1200]\n        \n        if len(self.image_paths) == 0:\n            print(f\"⚠️ No .nii or .nii.gz files found in {data_dir}\")\n        else:\n            print(f\"✅ Found {len(self.image_paths)} images in {data_dir}\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        try:\n            img_nii = nib.load(img_path)\n            img_data = img_nii.get_fdata(dtype=np.float32)\n\n            img_resized = resize(img_data, (self.img_size, self.img_size, self.img_size), mode='constant', anti_aliasing=True)\n\n            min_val, max_val = np.min(img_resized), np.max(img_resized)\n            if max_val - min_val > 1e-6:\n                img_normalized = 2 * (img_resized - min_val) / (max_val - min_val) - 1\n            else:\n                img_normalized = np.zeros_like(img_resized)\n\n            if self.augmentation:\n                if np.random.rand() > 0.5:\n                    img_normalized = np.flip(img_normalized, axis=0).copy()\n                if np.random.rand() > 0.5:\n                    img_normalized = np.flip(img_normalized, axis=1).copy()\n                if np.random.rand() > 0.5:\n                    img_normalized = np.flip(img_normalized, axis=2).copy()\n                scale_factor = np.random.uniform(0.9, 1.1)\n                img_normalized *= scale_factor\n\n            image_tensor = torch.from_numpy(img_normalized).float().unsqueeze(0) # (1, D, H, W)\n\n            return image_tensor\n\n        except Exception as e:\n            warnings.warn(f\"Error loading {img_path}: {e}\")\n            return torch.zeros((1, self.img_size, self.img_size, self.img_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:22.140649Z","iopub.execute_input":"2025-05-09T02:23:22.140912Z","iopub.status.idle":"2025-05-09T02:23:22.150397Z","shell.execute_reply.started":"2025-05-09T02:23:22.140885Z","shell.execute_reply":"2025-05-09T02:23:22.149712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainset = MyMRIDataset(root_dir=DATASET_ROOT,\n#                         img_size=IMG_SIZE,\n#                         augmentation=True)\ntrainset = MyMRIDataset(data_dir=DATASET_ROOT, img_size=IMG_SIZE, augmentation=False)\n\nif len(trainset) > 0:\n    train_loader = DataLoader(trainset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              num_workers=workers,\n                              pin_memory=True, # Helps speed up data transfer to GPU\n                              drop_last=True) # Drop last incomplete batch\n    print(f\"DataLoader created with {len(train_loader)} batches.\")\nelse:\n    print(\"Dataset is empty. Cannot create DataLoader. Please check dataset path and participants file.\")\n    # Stop execution or handle error appropriately\n    train_loader = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:22.151218Z","iopub.execute_input":"2025-05-09T02:23:22.15148Z","iopub.status.idle":"2025-05-09T02:23:22.301559Z","shell.execute_reply.started":"2025-05-09T02:23:22.151455Z","shell.execute_reply":"2025-05-09T02:23:22.300899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inf_train_gen(data_loader):\n    \"\"\" Creates an infinite generator from a DataLoader \"\"\"\n    if data_loader is None:\n        raise ValueError(\"DataLoader is not initialized.\")\n    while True:\n        for images in data_loader:\n            yield images\n\ndef calc_gradient_penalty(model, x, x_gen, w=LAMBDA):\n    \"\"\"WGAN-GP gradient penalty. Input tensors x and x_gen should be on the correct device.\"\"\"\n    assert x.size() == x_gen.size(), \"real and sampled sizes do not match\"\n    alpha_size = (x.size(0),) + (1,) * (x.dim() - 1)\n    # alpha = torch.rand(alpha_size, device=x.device) # More direct way\n    alpha_t = torch.cuda.FloatTensor if x.is_cuda else torch.Tensor\n    alpha = alpha_t(*alpha_size).uniform_().to(x.device)\n\n    # Use detach() to avoid backpropagating through alpha calculation itself\n    x_hat = (x.data * alpha + x_gen.data * (1 - alpha)).requires_grad_(True)\n\n    def eps_norm(tensor):\n        tensor = tensor.view(len(tensor), -1)\n        # Add _eps inside sqrt for stability\n        return (tensor * tensor).sum(dim=1).add_(_eps).sqrt()\n\n    def bi_penalty(tensor):\n        return (tensor - 1) ** 2\n\n    # Calculate gradients of model output w.r.t. interpolated inputs\n    pred_hat = model(x_hat)\n    # grad_outputs = torch.ones(pred_hat.size(), device=x.device) # Previous attempt\n    # Use .sum() as in the reference implementation to avoid shape issues\n    grad_xhat = autograd.grad(pred_hat.sum(), x_hat,\n                              # grad_outputs=grad_outputs, # Removed explicit grad_outputs\n                              create_graph=True, # Keep graph for second derivative\n                              retain_graph=True, # Retain graph needed for subsequent backward passes\n                              only_inputs=True)[0]\n\n    penalty = w * bi_penalty(eps_norm(grad_xhat)).mean()\n    return penalty","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:22.304004Z","iopub.execute_input":"2025-05-09T02:23:22.304221Z","iopub.status.idle":"2025-05-09T02:23:22.310797Z","shell.execute_reply.started":"2025-05-09T02:23:22.304204Z","shell.execute_reply":"2025-05-09T02:23:22.310093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport os\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\n\n#***********************************************\n#Encoder and Discriminator has same architecture\n#***********************************************\nclass Discriminator(nn.Module):\n    def __init__(self, channel=512,out_class=1,is_dis =True):\n        super(Discriminator, self).__init__()\n        self.is_dis=is_dis\n        self.channel = channel\n        n_class = out_class \n        \n        self.conv1 = nn.Conv3d(1, channel//8, kernel_size=4, stride=2, padding=1)\n        self.conv2 = nn.Conv3d(channel//8, channel//4, kernel_size=4, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm3d(channel//4)\n        self.conv3 = nn.Conv3d(channel//4, channel//2, kernel_size=4, stride=2, padding=1)\n        self.bn3 = nn.BatchNorm3d(channel//2)\n        self.conv4 = nn.Conv3d(channel//2, channel, kernel_size=4, stride=2, padding=1)\n        self.bn4 = nn.BatchNorm3d(channel)\n        self.conv5 = nn.Conv3d(channel, n_class, kernel_size=4, stride=1, padding=0)\n        \n    def forward(self, x, _return_activations=False):\n        h1 = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n        h2 = F.leaky_relu(self.bn2(self.conv2(h1)), negative_slope=0.2)\n        h3 = F.leaky_relu(self.bn3(self.conv3(h2)), negative_slope=0.2)\n        h4 = F.leaky_relu(self.bn4(self.conv4(h3)), negative_slope=0.2)\n        h5 = self.conv5(h4)\n        output = h5\n        \n        return output\n    \nclass Code_Discriminator(nn.Module):\n    def __init__(self, code_size=100,num_units=750):\n        super(Code_Discriminator, self).__init__()\n        n_class = 1\n        self.l1 = nn.Sequential(nn.Linear(code_size, num_units),\n                                nn.BatchNorm1d(num_units),\n                                nn.LeakyReLU(0.2,inplace=True))\n        self.l2 = nn.Sequential(nn.Linear(num_units, num_units),\n                                nn.BatchNorm1d(num_units),\n                                nn.LeakyReLU(0.2,inplace=True))\n        self.l3 = nn.Linear(num_units, 1)\n        \n    def forward(self, x):\n        h1 = self.l1(x)\n        h2 = self.l2(h1)\n        h3 = self.l3(h2)\n        output = h3\n            \n        return output\n\nclass Generator(nn.Module):\n    def __init__(self, noise:int=100, channel:int=64):\n        super(Generator, self).__init__()\n        _c = channel\n\n        self.relu = nn.ReLU()\n        self.noise = noise\n        self.tp_conv1 = nn.ConvTranspose3d(noise, _c*8, kernel_size=4, stride=1, padding=0, bias=False)\n        self.bn1 = nn.BatchNorm3d(_c*8)\n        \n        self.tp_conv2 = nn.Conv3d(_c*8, _c*4, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(_c*4)\n        \n        self.tp_conv3 = nn.Conv3d(_c*4, _c*2, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(_c*2)\n        \n        self.tp_conv4 = nn.Conv3d(_c*2, _c, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn4 = nn.BatchNorm3d(_c)\n        \n        self.tp_conv5 = nn.Conv3d(_c, 1, kernel_size=3, stride=1, padding=1, bias=False)\n        \n    def forward(self, noise):\n\n        noise = noise.view(-1,self.noise,1,1,1)\n        h = self.tp_conv1(noise)\n        h = self.relu(self.bn1(h))\n        \n        # h = F.upsample(h,scale_factor = 2)\n        h = F.interpolate(h, scale_factor=2, mode='trilinear', align_corners=False)\n        h = self.tp_conv2(h)\n        h = self.relu(self.bn2(h))\n     \n        # h = F.upsample(h,scale_factor = 2)\n        h = F.interpolate(h, scale_factor=2, mode='trilinear', align_corners=False)\n        h = self.tp_conv3(h)\n        h = self.relu(self.bn3(h))\n\n        # h = F.upsample(h,scale_factor = 2)\n        h = F.interpolate(h, scale_factor=2, mode='trilinear', align_corners=False)\n        h = self.tp_conv4(h)\n        h = self.relu(self.bn4(h))\n\n        # h = F.upsample(h,scale_factor = 2)\n        h = F.interpolate(h, scale_factor=2, mode='trilinear', align_corners=False)\n        h = self.tp_conv5(h)\n\n        h = torch.tanh(h) # Use torch.tanh instead of F.tanh for consistency\n\n        return h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:22.311545Z","iopub.execute_input":"2025-05-09T02:23:22.311838Z","iopub.status.idle":"2025-05-09T02:23:22.329096Z","shell.execute_reply.started":"2025-05-09T02:23:22.311815Z","shell.execute_reply":"2025-05-09T02:23:22.328543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    G = Generator(noise=latent_dim).to(device)\n    # Assuming Code_Discriminator takes code_size and num_units\n    CD = Code_Discriminator(code_size=latent_dim, num_units=4096).to(device)\n    D = Discriminator(is_dis=True).to(device) # Image Discriminator\n    # Encoder E has the same architecture as D but outputs latent_dim\n    E = Discriminator(out_class=latent_dim, is_dis=False).to(device)\n    print(\"Models initialized successfully.\")\nexcept NameError:\n    print(\"Model classes not found. Make sure Model_alphaWGAN.py is loaded correctly.\")\n    # Stop execution\n    raise SystemExit(\"Model loading failed.\")\nexcept Exception as e:\n    print(f\"Error initializing models: {e}\")\n    raise SystemExit(\"Model initialization failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:22.329815Z","iopub.execute_input":"2025-05-09T02:23:22.329984Z","iopub.status.idle":"2025-05-09T02:23:23.588746Z","shell.execute_reply.started":"2025-05-09T02:23:22.32997Z","shell.execute_reply":"2025-05-09T02:23:23.588043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"g_optimizer = optim.Adam(G.parameters(), lr=LR, betas=(BETA1, BETA2))\nd_optimizer = optim.Adam(D.parameters(), lr=LR, betas=(BETA1, BETA2))\ne_optimizer = optim.Adam(E.parameters(), lr=LR, betas=(BETA1, BETA2))\ncd_optimizer = optim.Adam(CD.parameters(), lr=LR, betas=(BETA1, BETA2))\n\ncriterion_l1 = nn.L1Loss().to(device)\n# WGAN-GP doesn't use BCE directly for D/G loss, but L1 is used for reconstruction\nprint(\"Optimizers and L1 Loss defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:23.589482Z","iopub.execute_input":"2025-05-09T02:23:23.589856Z","iopub.status.idle":"2025-05-09T02:23:25.986062Z","shell.execute_reply.started":"2025-05-09T02:23:23.589829Z","shell.execute_reply":"2025-05-09T02:23:25.985289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n# Giả sử các thư viện khác như nib và các model (G, D, E, CD), criterion, optimizers đã được import và định nghĩa ở trên.\n# Giả sử các hằng số như TOTAL_ITER, BATCH_SIZE, latent_dim, VISUALIZATION_INTERVAL, SAVE_INTERVAL,\n# IMAGE_SAVE_DIR, CHECKPOINT_DIR, device đã được định nghĩa.\n# Giả sử train_loader và inf_train_gen đã được tạo.\n\n# --- Phần code training của bạn ở đây ---\n# ... (bao gồm định nghĩa model, optimizers, loss functions) ...\n\nif train_loader:\n    gen_load = inf_train_gen(train_loader)\n    print(\"Starting training...\")\n    for iteration in range(TOTAL_ITER):\n        # ---------------------------------------------\n        # Train Encoder (E) and Generator (G)\n        # ---------------------------------------------\n        for p in D.parameters(): p.requires_grad = False\n        for p in CD.parameters(): p.requires_grad = False\n        for p in E.parameters(): p.requires_grad = True\n        for p in G.parameters(): p.requires_grad = True\n        # for p in D.parameters() if not isinstance(D, nn.DataParallel) else D.module.parameters(): p.requires_grad = False\n        # for p in CD.parameters() if not isinstance(CD, nn.DataParallel) else CD.module.parameters(): p.requires_grad = False\n        # for p in E.parameters() if not isinstance(E, nn.DataParallel) else E.module.parameters(): p.requires_grad = True\n        # for p in G.parameters() if not isinstance(G, nn.DataParallel) else G.module.parameters(): p.requires_grad = True\n\n        for _ in range(g_iter): # Typically g_iter is 1\n            g_optimizer.zero_grad()\n            e_optimizer.zero_grad()\n\n            real_images = next(gen_load).to(device)\n            _batch_size = real_images.size(0)\n            if _batch_size != BATCH_SIZE:\n                print(f\"Warning: Mismatched batch size ({_batch_size}), skipping iteration.\")\n                continue\n\n            # Tạo z random\n            z_rand = torch.randn(_batch_size, latent_dim, device=device)\n            # Ảnh thật -E--> z^\n            z_hat = E(real_images).view(_batch_size, -1)\n            x_hat = G(z_hat)\n            x_rand = G(z_rand)\n\n            loss_l1 = criterion_l1(x_hat, real_images)\n            loss_cd = -CD(z_hat).mean()\n            loss_d_recons = -D(x_hat).mean()\n            loss_d_fake = -D(x_rand).mean()\n            loss_ge = 10 * loss_l1 + loss_cd + loss_d_recons + loss_d_fake\n            loss_ge.backward()\n            g_optimizer.step()\n            e_optimizer.step()\n\n\n        # ---------------------------------------------\n        # Train Discriminator (D)\n        # ---------------------------------------------\n        for p in D.parameters(): p.requires_grad = True\n        for p in CD.parameters(): p.requires_grad = False\n        for p in E.parameters(): p.requires_grad = False\n        for p in G.parameters(): p.requires_grad = False\n\n        for _ in range(d_iter): # Typically d_iter is 1 or 5\n            d_optimizer.zero_grad()\n            real_images_d = next(gen_load).to(device) # Lấy batch mới cho D\n            _batch_size_d = real_images_d.size(0)\n            if _batch_size_d != BATCH_SIZE:\n                continue\n\n            z_rand_d = torch.randn(_batch_size_d, latent_dim, device=device)\n            with torch.no_grad():\n                z_hat_d = E(real_images_d).view(_batch_size_d, -1)\n                x_hat_d = G(z_hat_d)\n                x_rand_d = G(z_rand_d)\n\n            d_real = D(real_images_d).mean()\n            d_recons = D(x_hat_d).mean()\n            d_fake = D(x_rand_d).mean()\n            gp_real_fake = calc_gradient_penalty(D, real_images_d.data, x_rand_d.data)\n            gp_real_recons = calc_gradient_penalty(D, real_images_d.data, x_hat_d.data)\n            loss_d = d_recons + d_fake - 2 * d_real + gp_real_fake + gp_real_recons\n            loss_d.backward()\n            d_optimizer.step()\n\n        # ---------------------------------------------\n        # Train Code Discriminator (CD)\n        # ---------------------------------------------\n        # (Code training CD giữ nguyên)\n        for p in D.parameters(): p.requires_grad = False\n        for p in CD.parameters(): p.requires_grad = True\n        for p in E.parameters(): p.requires_grad = False\n        for p in G.parameters(): p.requires_grad = False\n\n        for _ in range(cd_iter): # Typically cd_iter is 1\n            cd_optimizer.zero_grad()\n            real_images_cd = next(gen_load).to(device) # Lấy batch mới cho CD\n            _batch_size_cd = real_images_cd.size(0)\n            if _batch_size_cd != BATCH_SIZE:\n                continue\n\n            z_rand_cd = torch.randn(_batch_size_cd, latent_dim, device=device)\n            with torch.no_grad():\n                z_hat_cd = E(real_images_cd).view(_batch_size_cd, -1)\n\n            cd_real = CD(z_hat_cd).mean()\n            cd_fake = CD(z_rand_cd).mean()\n            gp_code = calc_gradient_penalty(CD, z_hat_cd.data, z_rand_cd.data)\n            loss_c = cd_fake - cd_real + gp_code\n            loss_c.backward()\n            cd_optimizer.step()\n\n\n        # ---------------------------------------------\n        # Logging and Visualization (IMPROVED)\n        # ---------------------------------------------\n        if (iteration + 1) % VISUALIZATION_INTERVAL == 0:\n            print(f'[{iteration+1}/{TOTAL_ITER}] | Loss_D: {loss_d.item():.3f} | Loss_G/E: {loss_ge.item():.3f} | Loss_C: {loss_c.item():.3f} | L1: {loss_l1.item():.3f}')\n\n            # Sử dụng no_grad để tránh tính toán gradient không cần thiết\n            with torch.no_grad():\n                # Lấy ảnh từ batch gần nhất dùng để train G/E hoặc D/CD\n                # Sử dụng real_images (từ G/E) hoặc real_images_d/real_images_cd\n                # Ở đây dùng real_images từ bước train G/E\n                real_img_vis = real_images[0].cpu().numpy().squeeze()\n                print(f\"--- Iteration {iteration+1} ---\")\n                print(f\"real_img_vis - Min: {np.min(real_img_vis):.4f}, Max: {np.max(real_img_vis):.4f}, Mean: {np.mean(real_img_vis):.4f}\")\n                \n                recons_img_vis = x_hat[0].cpu().numpy().squeeze()\n\n                # Tạo ảnh fake mới để visualize\n                z_vis = torch.randn(1, latent_dim, device=device)\n                fake_img_vis = G(z_vis)[0].cpu().numpy().squeeze()\n\n                images_to_plot = [real_img_vis, recons_img_vis, fake_img_vis]\n                titles = ['Real', 'Reconstructed', 'Generated']\n\n                # Kiểm tra xem ảnh có phải 3D không\n                if real_img_vis.ndim != 3:\n                   print(f\"Warning: Image data is not 3D (shape: {real_img_vis.shape}). Skipping visualization for iteration {iteration+1}.\")\n                   continue # Bỏ qua nếu không phải ảnh 3D\n\n                # Tạo figure với 3 hàng (Real, Recons, Fake) và 3 cột (Sagittal, Coronal, Axial)\n                fig, axes = plt.subplots(3, 3, figsize=(15, 15)) # Tăng kích thước để dễ nhìn\n                fig.suptitle(f'Iteration {iteration+1}', fontsize=16)\n\n                for i, (img_data, title_prefix) in enumerate(zip(images_to_plot, titles)):\n                    # Kiểm tra lại lần nữa phòng trường hợp ảnh recon/fake có vấn đề\n                    if img_data is None or img_data.ndim != 3:\n                        print(f\"Warning: Invalid data for {title_prefix}. Skipping row.\")\n                        for j in range(3):\n                             axes[i, j].text(0.5, 0.5, 'Invalid Data', ha='center', va='center')\n                             axes[i, j].axis('off')\n                        continue\n\n                    # Lấy kích thước (giả sử là Depth, Height, Width hoặc tương tự)\n                    # Quan trọng: Thứ tự các chiều này phải đúng với dữ liệu của bạn\n                    # Nếu ảnh của bạn là (Width, Height, Depth), bạn cần điều chỉnh các lát cắt bên dưới\n                    dim1, dim2, dim3 = img_data.shape\n                    mid1, mid2, mid3 = dim1 // 2, dim2 // 2, dim3 // 2\n\n                    # --- Cột 1: Sagittal (Cắt dọc theo chiều thứ nhất) ---\n                    ax = axes[i, 0]\n                    # Hiển thị lát cắt ở giữa chiều thứ nhất\n                    # .T để transpose cho phù hợp với cách imshow hiển thị\n                    sagittal_slice = img_data[mid1, :, :]\n                    print(f\"Sagittal Slice - Min: {np.min(sagittal_slice):.4f}, Max: {np.max(sagittal_slice):.4f}\")\n                    # Thử set vmin, vmax tường minh\n                    ax.imshow(sagittal_slice.T, cmap=\"gray\", origin=\"lower\", vmin=-1, vmax=1)\n                    ax.set_title(f\"{title_prefix} - Sagittal (X={mid1})\")\n                    ax.axis(\"off\")\n                    # Đặt ylabel cho hàng đầu tiên của mỗi ảnh\n                    if i == 0: ax.set_ylabel(\"Real\", fontsize=14, rotation=0, labelpad=40, va='center')\n                    if i == 1: ax.set_ylabel(\"Reconstructed\", fontsize=14, rotation=0, labelpad=40, va='center')\n                    if i == 2: ax.set_ylabel(\"Generated\", fontsize=14, rotation=0, labelpad=40, va='center')\n\n\n                    # --- Cột 2: Coronal (Cắt dọc theo chiều thứ hai) ---\n                    ax = axes[i, 1]\n                    coronal_slice = img_data[:, mid2, :]\n                    ax.imshow(coronal_slice.T, cmap=\"gray\", origin=\"lower\")\n                    ax.set_title(f\"{title_prefix} - Coronal (Y={mid2})\")\n                    ax.axis(\"off\")\n                    # --- Cột 3: Axial (Cắt ngang theo chiều thứ ba) ---\n                    ax = axes[i, 2]\n                    axial_slice = img_data[:, :, mid3]\n                    ax.imshow(axial_slice.T, cmap=\"gray\", origin=\"lower\")\n                    ax.set_title(f\"{title_prefix} - Axial (Z={mid3})\")\n                    ax.axis(\"off\")\n\n                plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Điều chỉnh layout để không bị chồng title\n\n                # Lưu hình ảnh\n                save_path = os.path.join(IMAGE_SAVE_DIR, f'iteration_{iteration+1:06d}.png')\n                plt.savefig(save_path)\n                plt.show() # Hiển thị plot (tùy chọn, có thể xóa nếu chạy non-interactive)\n                plt.close(fig) # Đóng figure để giải phóng bộ nhớ\n\n        # ---------------------------------------------\n        # Model Checkpointing\n        # ---------------------------------------------\n        # (Code lưu model giữ nguyên)\n        if (iteration + 1) % SAVE_INTERVAL == 0:\n            g_path = os.path.join(CHECKPOINT_DIR, f'G_iter{iteration+1}.pth')\n            d_path = os.path.join(CHECKPOINT_DIR, f'D_iter{iteration+1}.pth')\n            e_path = os.path.join(CHECKPOINT_DIR, f'E_iter{iteration+1}.pth')\n            cd_path = os.path.join(CHECKPOINT_DIR, f'CD_iter{iteration+1}.pth')\n            torch.save(G.state_dict(), g_path)\n            torch.save(D.state_dict(), d_path)\n            torch.save(E.state_dict(), e_path)\n            torch.save(CD.state_dict(), cd_path)\n            print(f'Models saved at iteration {iteration+1}')\n\n    print(\"Training finished.\")\nelse:\n    print(\"Training cannot start because DataLoader was not initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:23:25.987057Z","iopub.execute_input":"2025-05-09T02:23:25.987753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Định nghĩa lại kiến trúc model\nG = Generator(noise=latent_dim).to(device)\nE = Discriminator(out_class=latent_dim, is_dis=False).to(device)\n\n# 2. Load weights\nG.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, '/kaggle/working/checkpoint_mydataset/G_iter25000.pth'), map_location=device))\nE.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, '/kaggle/working/checkpoint_mydataset/E_iter25000.pth'), map_location=device))\n\n# 3. Chuyển sang eval mode\nG.eval()\nE.eval()\n\n# 4. Sinh ảnh mới\nwith torch.no_grad():\n    z_rand = torch.randn(1, latent_dim, device=device)\n    # z_rand = torch.randn(_batch_size, latent_dim, device=device)\n    gen_img = G(z_rand)[0].cpu().numpy().squeeze()  # shape (D,H,W) hoặc (C,D,H,W)\n\n# 5. Tái tạo từ ảnh thật (ví dụ ảnh đầu tiên trong dataset)\nsample = next(iter(train_loader))[0:1].to(device)  # batch 1\nwith torch.no_grad():\n    z_hat = E(sample).view(1, -1)\n    rec_img = G(z_hat)[0].cpu().numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gen_img.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Nếu gen_img là torch.Tensor, chuyển sang NumPy\nif isinstance(gen_img, torch.Tensor):\n    gen_img = gen_img.detach().cpu().numpy()\n\n# Vẽ lát sagittal ở giữa\nmid1 = gen_img.shape[0] // 2\nplt.imshow(gen_img[ mid1,: , :].T, cmap='gray', origin='lower')\nplt.title('Generated Sagittal')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Nếu gen_img là torch.Tensor, chuyển sang NumPy\nif isinstance(rec_img, torch.Tensor):\n    rec_img = rec_img.detach().cpu().numpy()\n\n# Bỏ chiều batch: (1, 64, 64, 64) → (64, 64, 64)\nrec_img = rec_img[0]\n\n# Vẽ lát sagittal ở giữa\nmid1 = rec_img.shape[0] // 2\nplt.imshow(rec_img[ mid1,: , :].T, cmap='gray', origin='lower')\nplt.title('Generated Sagittal')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(rec_img[ mid1,: , :].T, cmap='gray', origin='lower')\nplt.title('Generated Sagittal')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Min:\", rec_img.min())\nprint(\"Max:\", rec_img.max())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Đưa sample về numpy (shape: 1, 1, 64, 64, 64)\nsample_np = sample[0, 0].cpu().numpy()\n\n# Kích thước: (64, 64, 64)\nsagittal = sample_np[sample_np.shape[0] // 2, :, :]\ncoronal  = sample_np[:, sample_np.shape[1] // 2, :]\naxial    = sample_np[:, :, sample_np.shape[2] // 2]\n\n# Plot\nfig, axs = plt.subplots(1, 3, figsize=(12, 4))\naxs[0].imshow(sagittal.T, cmap='gray', origin='lower')\naxs[0].set_title('Sagittal Slice')\naxs[1].imshow(coronal.T, cmap='gray', origin='lower')\naxs[1].set_title('Coronal Slice')\naxs[2].imshow(axial.T, cmap='gray', origin='lower')\naxs[2].set_title('Axial Slice')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}